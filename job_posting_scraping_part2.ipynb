{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8e58d7",
   "metadata": {},
   "source": [
    "# Job Posting from LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c63f27",
   "metadata": {},
   "source": [
    "This is the code provides the script for scraping job posting including job description, skill set, number of applied applicants, type of workplace (Remote, On-site, Hybrid), job level, job type (Contract, Full-time, Part-time, Internship etc.), industry of the hiring company, how long it was posted. \n",
    "\n",
    "This is the second step of job market analysis provided in the article: https://orlovtsu.github.io/job_postings_analysis.html.\n",
    "\n",
    "If you use this code for scraping data from LinkedIn be aware about the LinkedIn Term of Use and be sure that you do not violate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2be21",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "1. Selenium is a tool for automating web browsers, and these modules allow you to interact with web elements, locate elements by various criteria, and simulate keyboard actions.\n",
    "2. BeautifulSoup module, which is used for parsing HTML and XML documents, provides convenient methods for extracting data from web pages.\n",
    "3. Pandas library is a powerful data manipulation and analysis tool. It provides data structures and functions for efficiently handling structured data.\n",
    "4. Time module provides functions for working with time-related operations, such as delays and timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6122b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbccd2c",
   "metadata": {},
   "source": [
    "## Initialization \n",
    "This next chunk assigns the path to the ChromeDriver executable to the variable chromedriver_path. The ChromeDriver is a separate executable that is required when using Selenium with Google Chrome. It acts as a bridge between the Selenium WebDriver and the Chrome browser, allowing automated interactions with the browser.\n",
    "\n",
    "In this case, the chromedriver_path is set to './chromedriver', indicating that the ChromeDriver executable is located in the current directory (denoted by '.') and its filename is chromedriver. The specific path may vary depending on the actual location of the ChromeDriver executable on your system.\n",
    "\n",
    "Make sure to provide the correct path to the ChromeDriver executable file in order for Selenium to work properly with Google Chrome.\n",
    "\n",
    "For more details: https://chromedriver.chromium.org/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0621a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = './chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5872377",
   "metadata": {},
   "source": [
    "The next chunk create an instance of ChromeOptions class from the Selenium webdriver module. ChromeOptions allows you to customize the behavior of the Chrome browser when it is launched. In this case, the --start-maximized argument is added to the options, which instructs Chrome to start in maximized window mode.\n",
    "\n",
    "This code also creates an instance of the webdriver.Chrome class, passing the options object and chromedriver_path as arguments. It initializes the Chrome webdriver, using the ChromeDriver executable located at chromedriver_path and applying the specified options for Chrome's behavior. Code sets an implicit wait time of 10 seconds for the driver object. The implicit wait instructs Selenium to wait for a certain amount of time when trying to locate elements on the web page. It allows the driver to wait for a specified duration before throwing a NoSuchElementException if the element is not immediately available. In this case, the implicit wait is set to 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42925648",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406e0b6",
   "metadata": {},
   "source": [
    "Following chunk opens the page where you should authorize and changes the scale of viewing page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ae858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the web page with the login and password fields\n",
    "# Enter your username and password to authenticate as a LinkedIn user\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "\n",
    "# To speed up the downloading process for scraping the page content, it is recommended to reduce the page scale to 25%.\n",
    "# This will result in faster download of the majority of the content you require.\n",
    "driver.execute_script(\"document.body.style.zoom = '25%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e70771",
   "metadata": {},
   "source": [
    "## Job Posting Scraping script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6397cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the job list from a CSV file\n",
    "job_list = pd.read_csv('job_list_all.csv')\n",
    "data = []\n",
    "\n",
    "# Iterate through each job in the job list starting from index 526\n",
    "for index, job in job_list[526:].iterrows():\n",
    "    URL = 'https://www.linkedin.com' + '/'.join(job['URL'].split('/')[0:4]) # Create the URL for each job\n",
    "    driver.get(URL)  # Open the URL in the web driver\n",
    "    time.sleep(randint(4,10))  # Pause for a random time between 4 and 10 seconds\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')# Parse the page source using BeautifulSoup\n",
    "    \n",
    "    job_location = job['Location']  # Extract job location from the job list\n",
    "    job_title = job['Job Title']  # Extract job title from the job list\n",
    "    company_name = job['Company Name']  # Extract company name from the job list\n",
    "\n",
    "    try:\n",
    "        no_longer_message =  soup.find('span', class_ = 'artdeco-inline-feedback__message').get_text(strip = True)\n",
    "        print(index, no_longer_message)\n",
    "        no_longer = 'No longer' in no_longer_message  # Check if the job posting is no longer available\n",
    "    except:\n",
    "        no_longer = False\n",
    "        \n",
    "    if not no_longer:\n",
    "        job_insights = soup.find_all('li', {'class': 'jobs-unified-top-card__job-insight'})# Find job insights\n",
    " \n",
    "        # Extract job type and job level from job insights\n",
    "        try:\n",
    "            job_type = job_insights[0].find('span').get_text(strip = True).split('路')[0]\n",
    "        except:\n",
    "            job_type = 'Not defined'\n",
    "        try:\n",
    "            job_level = job_insights[0].find('span').get_text(strip = True).split('路')[1]\n",
    "        except:\n",
    "            job_level = 'Not defined'\n",
    "\n",
    "        # Extract company size and job industry from job insights\n",
    "        try:\n",
    "            company_size = job_insights[1].find('span').get_text(strip = True).split('路')[0]\n",
    "        except:\n",
    "            company_size = 'Not defined'\n",
    "        try:\n",
    "            job_industry = job_insights[1].find('span').get_text(strip = True).split('路')[1]\n",
    "        except:\n",
    "            job_industry = 'Not defined'\n",
    "\n",
    "        # Extract number of job applicants\n",
    "        try:\n",
    "            for insight in job_insights[2:]:  \n",
    "                text = insight.find('span').get_text(strip = True)\n",
    "                if 'applicants' in text:\n",
    "                    job_applicants = text.split(' ')[5]\n",
    "                    break\n",
    "        except:\n",
    "            try:\n",
    "                job_applicants = soup.find('span', {'class': 'jobs-unified-top-card__applicant-count'}).get_text(strip = True).split(' ')[0]\n",
    "            except:\n",
    "                try:\n",
    "                    span_element = soup.find('span', class_='jobs-unified-top-card__subtitle-secondary-grouping')\n",
    "                    job_applicants = span_element.find('span', class_='jobs-unified-top-card__bullet').get_text(strip=True)\n",
    "                except:\n",
    "                    job_applicants = 'Not defined'\n",
    "        \n",
    "        try:\n",
    "            posted = soup.find('span', {'class': 'jobs-unified-top-card__posted-date'}).get_text(strip = True)\n",
    "        except:\n",
    "            posted = 'Not defined'\n",
    "        try:\n",
    "            workplacetype = soup.find('span', {'class': 'jobs-unified-top-card__workplace-type'}).get_text(strip = True)\n",
    "        except:\n",
    "            workplacetype = 'Not defined'\n",
    "            \n",
    "        # Click the \"Skills\" button to view job skills\n",
    "        try:\n",
    "            button_skills = driver.find_element(by=By.CLASS_NAME, value = 'jobs-unified-top-card__job-insight-text-button')\n",
    "            button_skills.click()\n",
    "            time.sleep(randint(4,10))\n",
    "            soup_skills = BeautifulSoup(driver.page_source, 'html.parser')   \n",
    "            skills = soup_skills.find_all('li', {'class': 'job-details-skill-match-status-list__unmatched-skill'})\n",
    "            skill_set = ''\n",
    "            for skill in skills:\n",
    "                div_element = skill.find('div', class_='display-flex')  # Example: Locating the div based on the 'display-flex' class\n",
    "                skill_text = div_element.get_text(strip=True)\n",
    "                skill_set = skill_set + skill_text + '; '\n",
    "\n",
    "            button_exit = driver.find_element(By.XPATH, \"//span[text()='Done']\")\n",
    "            button_exit.click()\n",
    "        except:\n",
    "            skill_set = ''\n",
    "        \n",
    "        # Click the \"More\" button to view full job description\n",
    "        try:\n",
    "            button_more = driver.find_element(by=By.CLASS_NAME, value = 'jobs-description__footer-button')\n",
    "            button_more.click()\n",
    "            time.sleep(randint(4,10))\n",
    "            soup_more = BeautifulSoup(driver.page_source, 'html.parser')  \n",
    "        except:\n",
    "            button_more = None\n",
    "                \n",
    "        # Extract job description \n",
    "        try:\n",
    "            description = soup.find('div', {'class': 'jobs-box__html-content'}).get_text(strip = True)\n",
    "        except:\n",
    "            description = 'Not defined'\n",
    "        \n",
    "        # Output for checking the progress\n",
    "        print(index, job_title, company_name)\n",
    "        \n",
    "        # Append the extracted job information to the data list\n",
    "        data.append({\n",
    "            'Job Title': job_title, #+\n",
    "            'Company Name': company_name, \n",
    "            'Company Size': company_size, \n",
    "            'Location': job_location, \n",
    "            'URL': URL, \n",
    "            'Workplace_Type': workplacetype, \n",
    "            'Posted': posted,\n",
    "            'Applicants': job_applicants,\n",
    "            'Industry': job_industry, \n",
    "            'Job level': job_level,\n",
    "            'Job type': job_type, \n",
    "            'Skillset': skill_set,\n",
    "            'Description': description\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6b87d",
   "metadata": {},
   "source": [
    "## Results saving\n",
    "Do not forget to save you results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a274a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the collected job data\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('job_postings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
