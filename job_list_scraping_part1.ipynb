{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0025fe57",
   "metadata": {},
   "source": [
    "# Job List from LinkedIn \n",
    "\n",
    "This is the code for scraping linkedin job list using LinkedIn job search engine which provide Job Title, Company Name, Location and URL of each job.\n",
    "\n",
    "This is the first step of job market analysis provided in the article: https://orlovtsu.github.io/job_postings_analysis.html.\n",
    "\n",
    "If you use this code for scraping data from LinkedIn be aware about the LinkedIn Term of Use and be sure that you do not violate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2731c3",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "1. Selenium is a tool for automating web browsers, and these modules allow you to interact with web elements, locate elements by various criteria, and simulate keyboard actions.\n",
    "2. BeautifulSoup module, which is used for parsing HTML and XML documents, provides convenient methods for extracting data from web pages.\n",
    "3. Pandas library is a powerful data manipulation and analysis tool. It provides data structures and functions for efficiently handling structured data.\n",
    "4. Time module provides functions for working with time-related operations, such as delays and timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6122b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77e2b1",
   "metadata": {},
   "source": [
    "## Initialization \n",
    "This next chunk assigns the path to the ChromeDriver executable to the variable chromedriver_path. The ChromeDriver is a separate executable that is required when using Selenium with Google Chrome. It acts as a bridge between the Selenium WebDriver and the Chrome browser, allowing automated interactions with the browser.\n",
    "\n",
    "In this case, the chromedriver_path is set to './chromedriver', indicating that the ChromeDriver executable is located in the current directory (denoted by '.') and its filename is chromedriver. The specific path may vary depending on the actual location of the ChromeDriver executable on your system.\n",
    "\n",
    "Make sure to provide the correct path to the ChromeDriver executable file in order for Selenium to work properly with Google Chrome.\n",
    "\n",
    "For more details: https://chromedriver.chromium.org/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0621a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = './chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a082b",
   "metadata": {},
   "source": [
    "The next chunk create an instance of ChromeOptions class from the Selenium webdriver module. ChromeOptions allows you to customize the behavior of the Chrome browser when it is launched. In this case, the --start-maximized argument is added to the options, which instructs Chrome to start in maximized window mode.\n",
    "\n",
    "This code also creates an instance of the webdriver.Chrome class, passing the options object and chromedriver_path as arguments. It initializes the Chrome webdriver, using the ChromeDriver executable located at chromedriver_path and applying the specified options for Chrome's behavior. Code sets an implicit wait time of 10 seconds for the driver object. The implicit wait instructs Selenium to wait for a certain amount of time when trying to locate elements on the web page. It allows the driver to wait for a specified duration before throwing a NoSuchElementException if the element is not immediately available. In this case, the implicit wait is set to 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42925648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options, executable_path=chromedriver_path)\n",
    "\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a09ed",
   "metadata": {},
   "source": [
    "Following chunk opens the page where you should authorize and changes the scale of viewing page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adff14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the web page with the login and password fields\n",
    "# Enter your username and password to authenticate as a LinkedIn user\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "\n",
    "# To speed up the downloading process for scraping the page content, it is recommended to reduce the page scale to 25%.\n",
    "# This will result in faster download of the majority of the content you require.\n",
    "driver.execute_script(\"document.body.style.zoom = '25%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f833cd9",
   "metadata": {},
   "source": [
    "## Job List Scraping script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2269471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the search query and location\n",
    "query = '\"BI\"'\n",
    "location = 'Canada'\n",
    "\n",
    "# Create an empty list to store the job data\n",
    "data = []\n",
    "\n",
    "# Loop through multiple pages\n",
    "for page_num in range(1, 40):\n",
    "    # Construct the URL for each page based on the query, location, and page number\n",
    "    url = f'https://www.linkedin.com/jobs/search/?keywords={query}&location={location}&start={25 * (page_num - 1)}'\n",
    "\n",
    "    # Decrease the page scale to 25% for faster content downloading    \n",
    "    driver.execute_script(\"document.body.style.zoom = '25%'\")\n",
    "\n",
    "    # Open the URL in the web driver\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Pause for a random time between 10 and 20 seconds\n",
    "    time.sleep(randint(10,20))  \n",
    "    \n",
    "    # Parse the page source using BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all job postings on the page\n",
    "    job_postings = soup.find_all('li', {'class': 'jobs-search-results__list-item'})\n",
    "    \n",
    "    # Print job titles for each job posting\n",
    "    for k, job in enumerate(job_postings):\n",
    "        try:                \n",
    "            print(k, ':', job.find('a', class_='job-card-list__title').get_text().strip())\n",
    "        except:\n",
    "            print(None)\n",
    "            \n",
    "    print('-----')\n",
    "    \n",
    "    # Extract relevant information from each job posting and store it in a list of dictionaries\n",
    "    for i in range(len(job_postings)):\n",
    "        j = 0\n",
    "        while ((job_postings[i].find('a', class_='job-card-list__title')== None) & (j < 10)):\n",
    "            print(i,': Attempt again -', j)\n",
    "            job_postings = soup.find_all('li', {'class': 'jobs-search-results__list-item'})\n",
    "            for k, job in enumerate(job_postings):\n",
    "                try:\n",
    "                    print(k, ':', job.find('a', class_='job-card-list__title').get_text().strip())\n",
    "                except:\n",
    "                    print(None)            \n",
    "            print('-----')\n",
    "            j += 1\n",
    "            if j == 4:\n",
    "                i += 1\n",
    "                j = 0\n",
    "        \n",
    "        job_posting = job_postings[i]\n",
    "\n",
    "        # Extract job title, company name, location, and URL from the job posting\n",
    "        try:\n",
    "            job_title = job_posting.find('a', class_='job-card-list__title').get_text().strip()\n",
    "        except AttributeError:\n",
    "            job_title = None\n",
    "        \n",
    "        # Extract company name. Depending of personal account settings, name of html tags and html structure may vary. \n",
    "        # If this line does not work, uncomment any of other lines to try again\n",
    "        try:\n",
    "            #company_name = job_posting.find('span', class_='job-card-container__primary-description ').get_text().strip()\n",
    "            company_name = job_posting.find('div', class_='job-card-container__company-name').text.strip()\n",
    "            #company_name = job_posting.find('span', class_='job-card-container__primary-description').get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            company_name = None\n",
    "        print(i, company_name)\n",
    "        try:\n",
    "            job_location = job_posting.find('li', class_='job-card-container__metadata-item').get_text().strip()\n",
    "        except AttributeError:\n",
    "            job_location = None\n",
    "\n",
    "        try:\n",
    "            URL = job_posting.find('a', class_='job-card-container__link', href=True).get('href')\n",
    "        except AttributeError:\n",
    "            URL = None\n",
    "\n",
    "        try:\n",
    "            job_next = job_postings[i+1].find('a', class_='job-card-list__title').get_text().strip()\n",
    "        except:\n",
    "            job_next = None\n",
    "        \n",
    "        if not job_next:\n",
    "            URL1 = '/'.join(URL.split('/')[0:4])\n",
    "            button = driver.find_element(by=By.XPATH, value = f\"//a[contains(@href, '{URL1}')]\")\n",
    "            button.click()\n",
    "      \n",
    "            time.sleep(2)\n",
    "            current_url = driver.current_url   \n",
    "            \n",
    "            driver.get(current_url)\n",
    "            \n",
    "            if ((i < len(job_postings)-1)):\n",
    "                time.sleep(randint(10,20))  # Wait for 20 seconds\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')        \n",
    "                j = 0\n",
    "                while ((job_postings[i+1].find('a', class_='job-card-list__title')== None) & (j < 10)):\n",
    "                    print(i,': Attempt again')\n",
    "                    job_postings = soup.find_all('li', {'class': 'jobs-search-results__list-item'})\n",
    "                    for k, job in enumerate(job_postings):\n",
    "                        try:\n",
    "                            print(k, ':', job.find('a', class_='job-card-list__title').get_text().strip())\n",
    "                        except:\n",
    "                            print(None)                \n",
    "                    print('-----')\n",
    "                    j += 1\n",
    "                    if j == 4:\n",
    "                        i += 1\n",
    "                        j = 0\n",
    "        # Append the extracted job information to the data list   \n",
    "        data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Company Name': company_name,\n",
    "            'Location': job_location,\n",
    "            'URL': URL,\n",
    "         })\n",
    "        \n",
    "        # Pause for 5 seconds\n",
    "        time.sleep(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837048f",
   "metadata": {},
   "source": [
    "Depending of an account and some personal settings, the strucure of HTML page may vary and this is why sometimes the previous chink may finish by any type of error. This is why, it should be tune sometimes and DataFrame should be saved to save data and not to repeat the scraping you already did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbe542",
   "metadata": {},
   "source": [
    "## Results saving\n",
    "Do not forget to save you results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3dabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the collected job data        \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('job_list_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
